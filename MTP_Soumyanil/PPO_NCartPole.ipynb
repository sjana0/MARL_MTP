{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a930cb5",
   "metadata": {},
   "source": [
    "Import All Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "809a94bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import yaml, argparse, os, json\n",
    "#from multiprocessing import Process\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "from scipy import signal\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Box, Discrete\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6b1d83",
   "metadata": {},
   "source": [
    "Set Run Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40d3e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_count = 0\n",
    "tb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d0edbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# env = gym.make(\"CartPole-v1\")\n",
    "# env_obs_size = env.observation_space.shape[0]\n",
    "# env_obs_size\n",
    "# dummy_input = torch.randn(1, env_obs_size, requires_grad=True)\n",
    "# dummy_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b889330a",
   "metadata": {},
   "source": [
    "Set Run Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0547c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 0\n",
    "# np.random.seed(seed)\n",
    "# random.seed(seed)\n",
    "# torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c28a6",
   "metadata": {},
   "source": [
    "Torch Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c76afb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegDataset(Dataset):\n",
    "    def __init__(self, df_x, df_y, device=\"cpu\", seed=0): \n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        self.seed = seed\n",
    "        self.device=device\n",
    "        x=df_x.values\n",
    "        y=df_y.values\n",
    "        self.x_train=torch.tensor(x,dtype=torch.float32).to(device=device)\n",
    "        self.y_train=torch.tensor(y,dtype=torch.float32).to(device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_train[idx],self.y_train[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1525f687",
   "metadata": {},
   "source": [
    "Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "348e77bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def count_vars(module):\n",
    "    return sum([np.prod(p.shape) for p in module.parameters()])\n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    return signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "def statistics_scalar(x, with_min_and_max=False):\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    global_sum, global_n = (np.sum(x), len(x))\n",
    "    mean = global_sum / global_n\n",
    "\n",
    "    global_sum_sq = np.sum((x - mean)**2)\n",
    "    std = np.sqrt(global_sum_sq / global_n)  # compute global std\n",
    "\n",
    "    if with_min_and_max:\n",
    "        global_min = np.min(x) if len(x) > 0 else np.inf\n",
    "        global_max = np.max(x) if len(x) > 0 else -np.inf\n",
    "        return mean, std, global_min, global_max\n",
    "    return mean, std\n",
    "\n",
    "class Configs(object):\n",
    "    def __init__(self, confg):\n",
    "        for key in confg:\n",
    "            setattr(self, key, confg[key])\n",
    "\n",
    "def get_exec_device():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using device:', device)\n",
    "    return device\n",
    "\n",
    "def calc_accuracy(lst_y, lst_y_h):\n",
    "    #print(lst_y, lst_y_h)\n",
    "    total_count = len(lst_y)\n",
    "    total_correct = 0\n",
    "    for y, y_h in zip(lst_y, lst_y_h):\n",
    "        #print(y, y_h)\n",
    "        if y == y_h:\n",
    "            total_correct += 1\n",
    "    return float(total_correct/total_count)\n",
    "\n",
    "def calc_mse(lst_y, lst_y_h):\n",
    "    np_lst_y = np.array(lst_y).flatten()\n",
    "    np_lst_y_h = np.array(lst_y_h).flatten()\n",
    "    return np.sum((np_lst_y - np_lst_y_h)**2)/len(np_lst_y_h)\n",
    "\n",
    "\n",
    "def evaluate_ppo(env, ppo, evl_epochs, evl_max_epoch_steps, device=\"cpu\"):\n",
    "    lst_ep_ret = []\n",
    "    lst_ep_len = []\n",
    "    for epoch in range(evl_epochs):\n",
    "        #print(epoch)\n",
    "        o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "        for t in range(evl_max_epoch_steps):\n",
    "            a_h = ppo.ac.evaluate(torch.as_tensor(o, dtype=torch.float32).to(device=device))#rule_list.get_arg_max([o])[0]\n",
    "            #print(ep_ret, \">>\", a_h)\n",
    "            next_o, r, d, _ = env.step(a_h)\n",
    "            o = next_o\n",
    "            terminal = d\n",
    "            ep_ret += float(r)\n",
    "            ep_len += 1\n",
    "            if terminal:\n",
    "                break\n",
    "        lst_ep_ret.append(ep_ret)\n",
    "        lst_ep_len.append(ep_len)\n",
    "        if epoch%100==0:\n",
    "            print(epoch, \"ppo ep ret\", ep_ret, ep_len)\n",
    "        o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "    print(\"Average ppo ep ret for \", evl_epochs, sum(lst_ep_ret)/len(lst_ep_ret))\n",
    "    return lst_ep_ret, lst_ep_len\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_list_as_csv(lst_x, path, name, columns=None):\n",
    "    #np.savetxt(path+\"/\"+name, np.array(lst_x), delimiter =\", \", fmt ='% s')\n",
    "    pd.DataFrame(np.transpose(np.array(lst_x)), columns=columns).to_csv(path_or_buf=path+\"/\"+name, sep=',')\n",
    "\n",
    "def write_to_tensorboard(lst_x, lst_y, name):\n",
    "    pass\n",
    "\n",
    "def read_config(file_name):\n",
    "    with open(file_name) as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    return config\n",
    "\n",
    "def update_config(config, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    gym.utils.seeding.np_random(seed)\n",
    "\n",
    "def create_dir(dir_name):\n",
    "    success=True\n",
    "    try:\n",
    "        os.makedirs(dir_name)\n",
    "    except:\n",
    "        success=False\n",
    "    return success\n",
    "\n",
    "def get_env_action_space_type(action_space):\n",
    "    if isinstance(action_space, Box):\n",
    "        return \"box\"\n",
    "    elif isinstance(action_space, Discrete):\n",
    "        return \"discrete\"\n",
    "\n",
    "def get_env_dims(env):\n",
    "    env_obs_size = 0\n",
    "    env_act_size = 0\n",
    "    env_obs_size = env.observation_space.shape[0]\n",
    "    if get_env_action_space_type(env.action_space)==\"box\":\n",
    "        env_act_size = env.action_space.shape[0]\n",
    "    elif get_env_action_space_type(env.action_space)==\"discrete\":\n",
    "        env_act_size = env.action_space.n\n",
    "    return env_obs_size, env_act_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7513a53e",
   "metadata": {},
   "source": [
    "asdasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb9065b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.nn_layers = nn.ModuleList()\n",
    "        self.nn_layers_act = []\n",
    "        self.x_m_1 = None\n",
    "        self.x_w_r = None\n",
    "        self.layers_size = len(layers)\n",
    "        \n",
    "        for i in range(0, len(layers), 2):\n",
    "            ly = layers[i]\n",
    "            act = layers[i+1]\n",
    "            self.nn_layers.append(ly)\n",
    "            self.nn_layers.append(act)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_m_1 = None        \n",
    "        #for ly, act in zip(self.nn_layers, self.nn_layers_act):\n",
    "        for i in range(0, self.layers_size, 2):\n",
    "            x_m_1 = x\n",
    "            ly = self.nn_layers[i]\n",
    "            act = self.nn_layers[i+1]\n",
    "            x = ly(x)\n",
    "            self.x_w_r = x\n",
    "            x = act(x)\n",
    "        self.x_m_1 = x_m_1\n",
    "        return x\n",
    "\n",
    "\n",
    "def construct_mlp(sizes, activation, output_activation=nn.Identity, device=\"cpu\"):\n",
    "    layers = []\n",
    "    print(\"MLP_Sizes\", sizes)\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    m__ = MLP(layers).to(device=device)\n",
    "    print(\"NN Model:\", m__, activation, output_activation)\n",
    "    return  m__\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def _distribution(self, obs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, obs, act=None):\n",
    "        pi = self._distribution(obs)\n",
    "        logp_a = None\n",
    "        if act is not None:\n",
    "            logp_a = self._log_prob_from_distribution(pi, act)\n",
    "        return pi, logp_a\n",
    "\n",
    "\n",
    "class MLPCategoricalActor(Actor):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.logits_net = construct_mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation, device=device)\n",
    "        self.nn_net = self.logits_net\n",
    "\n",
    "    def _distribution(self, obs):\n",
    "        logits = self.logits_net(obs)\n",
    "        return Categorical(logits=logits)\n",
    "    \n",
    "    def arg_max(self, obs):\n",
    "        logits = self.logits_net(obs)\n",
    "        return logits.argmax(dim=-1)\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        return pi.log_prob(act)\n",
    "\n",
    "\n",
    "class MLPGaussianActor(Actor):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        log_std = -0.5 * np.ones(act_dim, dtype=np.float32)\n",
    "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n",
    "        self.mu_net = construct_mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation, device=device)\n",
    "        self.nn_net = self.mu_net\n",
    "\n",
    "    def _distribution(self, obs):\n",
    "        mu = self.mu_net(obs)\n",
    "        std = torch.exp(self.log_std.to(device=device))\n",
    "        return Normal(mu, std)\n",
    "    \n",
    "    def arg_max(self, obs):\n",
    "        mu = self.mu_net(obs)\n",
    "        #std = torch.exp(self.log_std)\n",
    "        return mu#.detach().cpu().numpy()#Normal(mu, std).sample()\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        return pi.log_prob(act).sum(axis=-1)    # Last axis sum needed for Torch Normal distribution\n",
    "\n",
    "\n",
    "class MLPCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, hidden_sizes, activation, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.v_net = construct_mlp([obs_dim] + list(hidden_sizes) + [1], activation, device=device)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return torch.squeeze(self.v_net(obs), -1) # Critical to ensure v has right shape.\n",
    "\n",
    "\n",
    "class MLPActorCritic(nn.Module):\n",
    "    def __init__(self, observation_space, action_space, device=\"cpu\", hidden_sizes=[64,64], activation=nn.ReLU, seed=100):\n",
    "        super().__init__()\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        self.seed = seed\n",
    "        obs_dim = observation_space.shape[0]\n",
    "\n",
    "        # policy builder depends on action space\n",
    "        if get_env_action_space_type(action_space)==\"box\":\n",
    "            self.pi = MLPGaussianActor(obs_dim, action_space.shape[0], hidden_sizes, activation, device=device)\n",
    "        elif get_env_action_space_type(action_space)==\"discrete\":\n",
    "            self.pi = MLPCategoricalActor(obs_dim, action_space.n, hidden_sizes, activation, device=device)\n",
    "\n",
    "        # build value function\n",
    "        self.v  = MLPCritic(obs_dim, hidden_sizes, activation, device=device)\n",
    "\n",
    "    def step(self, obs):\n",
    "        with torch.no_grad():\n",
    "            pi = self.pi._distribution(obs)\n",
    "            a = pi.sample()\n",
    "            logp_a = self.pi._log_prob_from_distribution(pi, a)\n",
    "            v = self.v(obs)\n",
    "        return a.cpu().numpy(), v.cpu().numpy(), logp_a.cpu().numpy()\n",
    "    \n",
    "    def get_prev_layer_val(self):\n",
    "        return self.pi.nn_net.x_m_1\n",
    "    \n",
    "    def evaluate(self, obs):\n",
    "        with torch.no_grad():\n",
    "            a = self.pi.arg_max(obs)\n",
    "        return a.cpu().numpy()\n",
    "\n",
    "    def act(self, obs):\n",
    "        return self.step(obs)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e622c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOBuffer:\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95, device=\"cpu\"):\n",
    "        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "        self.device=device\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "        \n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "        adv_mean, adv_std = statistics_scalar(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
    "                    adv=self.adv_buf, logp=self.logp_buf)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32).to(self.device) for k,v in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a5b2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO():\n",
    "    def __init__(self, env, actor_critic=MLPActorCritic, ac_kwargs=dict(), seed=0, \n",
    "            steps_per_epoch=4000, epochs=50, gamma=0.99, clip_ratio=0.2, pi_lr=3e-4,\n",
    "            vf_lr=1e-3, train_pi_iters=80, train_v_iters=80, lam=0.97, max_ep_len=1000,\n",
    "            target_kl=0.01, logger_kwargs=dict(), save_freq=10, exp_name=\".\", exp_dir=\".\", device=\"cpu\", act_fn=nn.ReLU, tb=None):\n",
    "        self.lst_avg_episode_ret = []\n",
    "        self.lst_test_traj = []\n",
    "        self.env = env\n",
    "        self.actor_critic = actor_critic\n",
    "        self.ac_kwargs = ac_kwargs\n",
    "        self.seed = seed\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.epochs = epochs\n",
    "        self.gamma = gamma\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.pi_lr = pi_lr\n",
    "        self.vf_lr = vf_lr\n",
    "        self.train_pi_iters = train_pi_iters\n",
    "        self.train_v_iters = train_v_iters\n",
    "        self.lam = lam\n",
    "        self.max_ep_len = max_ep_len\n",
    "        self.target_kl = target_kl\n",
    "        self.logger_kwargs = logger_kwargs\n",
    "        self.save_freq = save_freq\n",
    "        self.exp_name = exp_name\n",
    "        self.exp_dir = exp_dir\n",
    "        self.device = device\n",
    "        self.act_fn = act_fn\n",
    "        self.tb = tb #TBWriter(exp_dir+\"/ppo\")\n",
    "        self.lst_train_obs = []\n",
    "\n",
    "        # Random seed\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # Instantiate environment\n",
    "        #self.env = self.env_fn()\n",
    "        self.env.seed(seed)\n",
    "        self.obs_dim = self.env.observation_space.shape\n",
    "        self.act_dim = self.env.action_space.shape\n",
    "        #print(\">>>>>>>>>>>>>>>>\", self.obs_dim, self.act_dim)\n",
    "\n",
    "        # Create actor-critic module\n",
    "        self.ac = self.actor_critic(self.env.observation_space, self.env.action_space, self.device, activation=self.act_fn, **ac_kwargs)\n",
    "\n",
    "        # Count variables\n",
    "        self.var_counts = tuple(count_vars(module) for module in [self.ac.pi, self.ac.v])\n",
    "\n",
    "        # Set up experience buffer\n",
    "        self.local_steps_per_epoch = self.steps_per_epoch #int(steps_per_epoch / num_procs())\n",
    "        self.buf = PPOBuffer(self.obs_dim, self.act_dim, self.local_steps_per_epoch, self.gamma, self.lam, device=self.device)\n",
    "\n",
    "        # Set up optimizers for policy and value function\n",
    "        self.pi_optimizer = Adam(self.ac.pi.parameters(), lr=pi_lr)\n",
    "        self.vf_optimizer = Adam(self.ac.v.parameters(), lr=vf_lr)\n",
    "\n",
    "    # Set up function for computing PPO policy loss\n",
    "    def compute_loss_pi(self, data):\n",
    "        obs, act, adv, logp_old = data['obs'], data['act'], data['adv'], data['logp']\n",
    "\n",
    "        # Policy loss\n",
    "        pi, logp = self.ac.pi(obs, act)\n",
    "        ratio = torch.exp(logp - logp_old)\n",
    "        clip_adv = torch.clamp(ratio, 1-self.clip_ratio, 1+self.clip_ratio) * adv\n",
    "        loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()\n",
    "\n",
    "        # Useful extra info\n",
    "        approx_kl = (logp_old - logp).mean().item()\n",
    "        ent = pi.entropy().mean().item()\n",
    "        clipped = ratio.gt(1+self.clip_ratio) | ratio.lt(1-self.clip_ratio)\n",
    "        clipfrac = torch.as_tensor(clipped, dtype=torch.float32).mean().item()\n",
    "        pi_info = dict(kl=approx_kl, ent=ent, cf=clipfrac)\n",
    "\n",
    "        return loss_pi, pi_info\n",
    "\n",
    "    # Set up function for computing value loss\n",
    "    def compute_loss_v(self, data):\n",
    "        obs, ret = data['obs'], data['ret']\n",
    "        return ((self.ac.v(obs) - ret)**2).mean()\n",
    "\n",
    "    def update(self, epoch):\n",
    "        data = self.buf.get()\n",
    "\n",
    "        pi_l_old, pi_info_old = self.compute_loss_pi(data)\n",
    "        pi_l_old = pi_l_old.item()\n",
    "        v_l_old = self.compute_loss_v(data).item()\n",
    "\n",
    "        pi_upd_iter = 0\n",
    "        total_loss_pi = 0.0\n",
    "        v_upd_iter = 0\n",
    "        total_loss_v = 0.0\n",
    "        # Train policy with multiple steps of gradient descent\n",
    "        for i in range(self.train_pi_iters):\n",
    "            pi_upd_iter += 1\n",
    "            self.pi_optimizer.zero_grad()\n",
    "            loss_pi, pi_info = self.compute_loss_pi(data)\n",
    "            total_loss_pi += loss_pi.item()\n",
    "            kl = pi_info['kl']\n",
    "            if kl > 1.5 * self.target_kl:\n",
    "                #logger.log('Early stopping at step %d due to reaching max kl.'%i)\n",
    "                break\n",
    "            loss_pi.backward()   # average grads across MPI processes\n",
    "            self.pi_optimizer.step()\n",
    "\n",
    "        # Value function learning\n",
    "        for i in range(self.train_v_iters):\n",
    "            v_upd_iter += 1\n",
    "            self.vf_optimizer.zero_grad()\n",
    "            loss_v = self.compute_loss_v(data)\n",
    "            total_loss_v += loss_v.item()\n",
    "            loss_v.backward()    # average grads across MPI processes\n",
    "            self.vf_optimizer.step()\n",
    "        \n",
    "        if self.tb is not None:\n",
    "            self.tb.writer.add_scalar('Train/KL', pi_info['kl'], epoch)    \n",
    "            self.tb.writer.add_scalar(\"Train/ENT\", pi_info_old['ent'], epoch)\n",
    "            self.tb.writer.add_scalar(\"Train/CF\", pi_info['cf'], epoch)\n",
    "            self.tb.writer.add_scalar(\"Train/pi_l_old\", pi_l_old, epoch)\n",
    "            self.tb.writer.add_scalar(\"Train/v_l_old\", v_l_old, epoch)\n",
    "            self.tb.writer.add_scalar(\"Train/pi_l_avg\", total_loss_pi/pi_upd_iter, epoch)\n",
    "            self.tb.writer.add_scalar(\"Train/v_l_avg\", total_loss_v/v_upd_iter, epoch)\n",
    "        # Log changes from update\n",
    "        kl, ent, cf = pi_info['kl'], pi_info_old['ent'], pi_info['cf']\n",
    "\n",
    "\n",
    "    def ppo_train(self):\n",
    "        #start_time = time.time()\n",
    "        o, ep_ret, ep_len = self.env.reset(), 0, 0\n",
    "\n",
    "        # Main loop: collect experience in env and update/log each epoch\n",
    "        lst_epochs_avg_ep_ret = []\n",
    "        episodes = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            total_epoch_ret = 0.0\n",
    "            epoch_ep_count = 0\n",
    "            for t in range(self.local_steps_per_epoch):\n",
    "                self.lst_train_obs.append(o)\n",
    "                a, v, logp = self.ac.step(torch.as_tensor(o, dtype=torch.float32).to(device=self.device))\n",
    "                next_o, r, d, _ = self.env.step(a)\n",
    "                ep_ret += r\n",
    "                ep_len += 1\n",
    "\n",
    "                # save and log\n",
    "                self.buf.store(o, a, r, v, logp)\n",
    "                \n",
    "                # Update obs (critical!)\n",
    "                o = next_o\n",
    "\n",
    "                timeout = ep_len == self.max_ep_len\n",
    "                terminal = d or timeout\n",
    "                epoch_ended = t==self.local_steps_per_epoch-1\n",
    "\n",
    "                if terminal or epoch_ended:\n",
    "                    if epoch_ended and not(terminal):\n",
    "                        print(epoch, 'Warning: trajectory cut off by epoch at %d steps.'%ep_len, flush=True, end=\"\")\n",
    "                    else:\n",
    "                        total_epoch_ret += ep_ret\n",
    "                        epoch_ep_count += 1\n",
    "                        if self.tb is not None:\n",
    "                            self.tb.writer.add_scalar('Train/Episode_Return', ep_ret, episodes)\n",
    "                            self.tb.writer.add_scalar('Train/Episode_Length', ep_len, episodes)\n",
    "                        episodes += 1\n",
    "                    # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                    if timeout or epoch_ended:\n",
    "                        _, v, _ = self.ac.step(torch.as_tensor(o, dtype=torch.float32).to(device=self.device))\n",
    "                    else:\n",
    "                        v = 0\n",
    "                    self.buf.finish_path(v)\n",
    "                    o, ep_ret, ep_len = self.env.reset(), 0, 0\n",
    "\n",
    "            # Perform PPO update!\n",
    "            self.update(epoch)\n",
    "\n",
    "            if epoch_ep_count == 0:\n",
    "                epoch_ep_count = 1\n",
    "            lst_epochs_avg_ep_ret.append(total_epoch_ret/epoch_ep_count)\n",
    "            print(\" Average EP return :\", total_epoch_ret/epoch_ep_count, total_epoch_ret, epoch_ep_count)\n",
    "            if self.tb is not None:\n",
    "                self.tb.writer.add_scalar('Train/Epoch_Return_Avg', total_epoch_ret/epoch_ep_count, epoch)    \n",
    "                self.tb.writer.add_histogram(\"Train/pi_model_weight_last_layer\", self.get_ppo_actor_weights_std()[0], epoch)\n",
    "                self.tb.writer.add_histogram(\"Train/pi_model_bias_last_layer\", self.get_ppo_actor_weights_std()[1], epoch)\n",
    "            if epoch%self.save_freq == 0:\n",
    "                self.save_ppo_model()        \n",
    "        self.save_ppo_model_final()\n",
    "        np.savetxt(self.exp_dir+\"/\"+\"lst_ppo_train_obs.csv\", np.array(self.lst_train_obs), delimiter =\", \", fmt ='% s')\n",
    "        return lst_epochs_avg_ep_ret\n",
    "            \n",
    "        #def ppo_test():\n",
    "    def save_ppo_model(self):\n",
    "        torch.save(self.ac.pi.state_dict(), self.exp_dir+\"/\"+\"ac_pi.mld\")\n",
    "        torch.save(self.ac.v.state_dict(), self.exp_dir+\"/\"+\"ac_v.mld\")\n",
    "    \n",
    "    def save_ppo_model_final(self):\n",
    "        torch.save(self.ac.pi.state_dict(), self.exp_dir+\"/\"+\"ac_pi.mld\")\n",
    "        torch.save(self.ac.v.state_dict(), self.exp_dir+\"/\"+\"ac_v.mld\")\n",
    "\n",
    "    def load_ppo_model(self):\n",
    "        self.ac.pi.load_state_dict(torch.load(self.exp_dir+\"/\"+\"ac_pi.mld\"))\n",
    "        self.ac.v.load_state_dict(torch.load(self.exp_dir+\"/\"+\"ac_v.mld\"))\n",
    "    \n",
    "    def get_ppo_actor_weights_std(self, layer_indx=-2):\n",
    "        weights = self.ac.pi.nn_net.nn_layers[layer_indx].weight.detach().cpu().numpy()\n",
    "        bias = self.ac.pi.nn_net.nn_layers[layer_indx].bias.detach().cpu().numpy()\n",
    "        if get_env_action_space_type(env.action_space)==\"box\":\n",
    "            std = torch.exp(self.ac.pi.log_std).detach().cpu().numpy()\n",
    "        else:\n",
    "            std = None\n",
    "        return weights, bias, std\n",
    "\n",
    "    def ppo_evaluation_dataset(self, epochs_count=0, max_epoch_steps=0):\n",
    "        print(\"Generating PPO Evaluation Dataset.....\")\n",
    "        lst_test_trajs = []\n",
    "        if os.path.exists(self.exp_dir+\"/\"+\"lst_ppo_train_obs.csv\"):\n",
    "            self.lst_train_obs = np.loadtxt(self.exp_dir+\"/\"+\"lst_ppo_train_obs.csv\", delimiter=\",\", dtype=float)\n",
    "            for o in (self.lst_train_obs):\n",
    "                a = self.ac.evaluate(torch.as_tensor(o, dtype=torch.float32).to(device=self.device))\n",
    "                prev_layer_val = self.ac.get_prev_layer_val().cpu().detach().cpu().numpy()\n",
    "                if get_env_action_space_type(self.env.action_space) == \"box\":\n",
    "                    lst_test_trajs.append(np.array(o.tolist()+prev_layer_val.tolist()+a.tolist()+[-1, -1, -1],dtype=float))\n",
    "                else:\n",
    "                    lst_test_trajs.append(np.array(o.tolist()+prev_layer_val.tolist()+[a[()]]+[-1, -1, -1],dtype=float))\n",
    "        \n",
    "        o, ep_ret, ep_len = self.env.reset(), 0, 0\n",
    "        for epoch in range(epochs_count):\n",
    "            for t in range(max_epoch_steps):\n",
    "                a = self.ac.evaluate(torch.as_tensor(o, dtype=torch.float32).to(device=self.device))\n",
    "                prev_layer_val = self.ac.get_prev_layer_val().cpu().detach().cpu().numpy()\n",
    "                #print(o, prev_layer_val, a)\n",
    "                next_o, r, d, _ = self.env.step(a)\n",
    "                ep_ret += r\n",
    "                ep_len += 1\n",
    "                if get_env_action_space_type(self.env.action_space) == \"box\":\n",
    "                    lst_test_trajs.append(np.array(o.tolist()+prev_layer_val.tolist()+a.tolist()+[epoch, ep_ret, ep_len],dtype=float))\n",
    "                else:\n",
    "                    lst_test_trajs.append(np.array(o.tolist()+prev_layer_val.tolist()+[a[()]]+[epoch, ep_ret, ep_len],dtype=float))\n",
    "                o = next_o\n",
    "                terminal = d\n",
    "                if terminal:\n",
    "                    break\n",
    "            o, ep_ret, ep_len = self.env.reset(), 0, 0\n",
    "        print(\"epochs :\", epoch, \"ep_ret :\", ep_ret, \"ep_len :\", ep_len)\n",
    "        return lst_test_trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fc195b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_train_ppo(env, ppo_hid_nodes, seed, ppo_dir, ppo_act_fn, ppo_epochs, ppo_epoch_steps, ppo_gamma):\n",
    "    device=get_exec_device()\n",
    "    env.seed(seed)\n",
    "    env_obs_size, env_act_size = get_env_dims(env)\n",
    "\n",
    "    ppo_tb = None #TBWriter(ppo_dir)\n",
    "    ppo = PPO(env, actor_critic=MLPActorCritic, ac_kwargs=dict(hidden_sizes=ppo_hid_nodes), gamma=ppo_gamma, seed=seed, steps_per_epoch=ppo_epoch_steps, epochs=ppo_epochs, exp_dir=ppo_dir, device=device, act_fn=ppo_act_fn, tb=ppo_tb)\n",
    "    \n",
    "    if not os.path.exists(ppo_dir+\"/\"+\"ac_pi.mld\"):\n",
    "        lst_epochs_avg_ep_ret = ppo.ppo_train()\n",
    "        lst_epochs = list(range(0,len(lst_epochs_avg_ep_ret)))\n",
    "        save_list_as_csv([lst_epochs, lst_epochs_avg_ep_ret], ppo_dir, \"ppo_original_train_epoch_ret.csv\", columns=[\"epoch\", \"avg ep ret\"])\n",
    "    else:\n",
    "        ppo.load_ppo_model()\n",
    "        \n",
    "    ppo_weights, ppo_biases, ppo_std = ppo.get_ppo_actor_weights_std()\n",
    "    return ppo, ppo_weights, ppo_biases, ppo_std\n",
    "\n",
    "\n",
    "def fn_load_ppo(env, ppo_hid_nodes, seed, run_dir, ppo_dir, ppo_act_fn, ppo_epochs, ppo_epoch_steps, ppo_gamma):\n",
    "    device=get_exec_device()\n",
    "    env.seed(seed)\n",
    "    env_obs_size, env_act_size = get_env_dims(env)\n",
    "    ppo_tb = None\n",
    "    ppo = PPO(env, actor_critic=MLPActorCritic, ac_kwargs=dict(hidden_sizes=ppo_hid_nodes), gamma=ppo_gamma, seed=seed, steps_per_epoch=ppo_epoch_steps, epochs=ppo_epochs, exp_dir=ppo_dir, device=device, act_fn=ppo_act_fn, tb=ppo_tb)\n",
    "    \n",
    "    ppo.load_ppo_model()\n",
    "    \n",
    "    ppo_weights, ppo_biases, ppo_std = ppo.get_ppo_actor_weights_std()\n",
    "    print(\"ppo_weights\", ppo_weights)\n",
    "    print(\"ppo_biases\", ppo_biases)\n",
    "    print(\"ppo_std\", ppo_std)\n",
    "    return ppo, ppo_weights, ppo_biases, ppo_std\n",
    "    \n",
    "\n",
    "        \n",
    "def fn_evaluate_ppo(env, ppo, ppo_dir, evl_epochs,evl_max_epoch_steps, ppo_tb, device):\n",
    "    print(\"Evaluating PPO... Epochs:\", evl_epochs, \"  Steps per Epoch:\", evl_max_epoch_steps)\n",
    "    if not os.path.exists(ppo_dir+\"/\"+\"lst_ppo_eval_ep_ret.csv\"):\n",
    "        lst_ep_ret_ppo, lst_ep_len_ppo = evaluate_ppo(env=env, ppo=ppo, evl_epochs=evl_epochs, evl_max_epoch_steps=evl_max_epoch_steps, device=device)\n",
    "        np.savetxt(ppo_dir+\"/\"+\"lst_ppo_eval_ep_ret.csv\", np.array(lst_ep_ret_ppo), delimiter =\", \", fmt ='% s')\n",
    "        np.savetxt(ppo_dir+\"/\"+\"lst_ppo_eval_ep_len.csv\", np.array(lst_ep_len_ppo), delimiter =\", \", fmt ='% s')\n",
    "    else:\n",
    "        lst_ep_ret_ppo = np.loadtxt(ppo_dir+\"/\"+\"lst_ppo_eval_ep_ret.csv\", delimiter=\",\", dtype=float)\n",
    "        lst_ep_len_ppo = np.loadtxt(ppo_dir+\"/\"+\"lst_ppo_eval_ep_len.csv\", delimiter=\",\", dtype=float)\n",
    "    \n",
    "    if ppo_tb is not None:\n",
    "        for ep, (ppo_ret, ppo_len) in enumerate(zip(lst_ep_ret_ppo, lst_ep_len_ppo)):\n",
    "            ppo_tb.writer.add_scalar('Evaluation/Episode_Return', ppo_ret, ep)\n",
    "            ppo_tb.writer.add_scalar('Evaluation/Episode_Length', ppo_len, ep)\n",
    "        ppo_tb.writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3fae67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config_File_Name :  config_NCartPole_1.yaml\n",
      "Using device: cpu\n",
      "Using device: cpu\n",
      "MLP_Sizes [16, 64, 64, 16]\n",
      "NN Model: MLP(\n",
      "  (nn_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=16, bias=True)\n",
      "    (5): Identity()\n",
      "  )\n",
      ") <class 'torch.nn.modules.activation.ReLU'> <class 'torch.nn.modules.linear.Identity'>\n",
      "MLP_Sizes [16, 64, 64, 1]\n",
      "NN Model: MLP(\n",
      "  (nn_layers): ModuleList(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "    (5): Identity()\n",
      "  )\n",
      ") <class 'torch.nn.modules.activation.ReLU'> <class 'torch.nn.modules.linear.Identity'>\n",
      "0 Warning: trajectory cut off by epoch at 12 steps. Average EP return : 12.906148867313917 3988.0 309\n",
      "1 Warning: trajectory cut off by epoch at 18 steps. Average EP return : 13.498305084745763 3982.0 295\n",
      "2 Warning: trajectory cut off by epoch at 1 steps. Average EP return : 13.695205479452055 3999.0 292\n",
      "3 Warning: trajectory cut off by epoch at 4 steps. Average EP return : 14.021052631578947 3996.0 285\n",
      "4 Warning: trajectory cut off by epoch at 12 steps. Average EP return : 15.279693486590038 3988.0 261\n",
      "5 Warning: trajectory cut off by epoch at 5 steps. Average EP return : 16.57676348547718 3995.0 241\n",
      "6 Warning: trajectory cut off by epoch at 15 steps. Average EP return : 17.869955156950674 3985.0 223\n",
      "7 Warning: trajectory cut off by epoch at 10 steps. Average EP return : 21.0 3990.0 190\n",
      "8 Warning: trajectory cut off by epoch at 1 steps. Average EP return : 24.384146341463413 3999.0 164\n",
      "9 Warning: trajectory cut off by epoch at 22 steps. Average EP return : 27.625 3978.0 144\n",
      "10 Warning: trajectory cut off by epoch at 6 steps. Average EP return : 25.935064935064936 3994.0 154\n",
      "11 Warning: trajectory cut off by epoch at 6 steps. Average EP return : 30.723076923076924 3994.0 130\n",
      "12 Warning: trajectory cut off by epoch at 40 steps. Average EP return : 34.13793103448276 3960.0 116\n",
      "13 Warning: trajectory cut off by epoch at 52 steps. Average EP return : 39.878787878787875 3948.0 99\n",
      "14 Warning: trajectory cut off by epoch at 18 steps. Average EP return : 45.25 3982.0 88\n",
      "15 Warning: trajectory cut off by epoch at 28 steps. Average EP return : 43.17391304347826 3972.0 92\n",
      "16 Warning: trajectory cut off by epoch at 61 steps. Average EP return : 43.285714285714285 3939.0 91\n",
      "17 Warning: trajectory cut off by epoch at 28 steps. Average EP return : 46.72941176470588 3972.0 85\n",
      "18 Warning: trajectory cut off by epoch at 12 steps. Average EP return : 51.12820512820513 3988.0 78\n",
      "19 Warning: trajectory cut off by epoch at 27 steps. Average EP return : 62.078125 3973.0 64\n",
      "20 Warning: trajectory cut off by epoch at 44 steps. Average EP return : 62.79365079365079 3956.0 63\n",
      "21 Warning: trajectory cut off by epoch at 25 steps. Average EP return : 64.11290322580645 3975.0 62\n",
      "22 Warning: trajectory cut off by epoch at 21 steps. Average EP return : 64.1774193548387 3979.0 62\n",
      "23 Warning: trajectory cut off by epoch at 27 steps. Average EP return : 64.08064516129032 3973.0 62\n",
      "24 Warning: trajectory cut off by epoch at 23 steps. Average EP return : 63.12698412698413 3977.0 63\n",
      "25 Warning: trajectory cut off by epoch at 59 steps. Average EP return : 69.14035087719299 3941.0 57\n",
      "26 Warning: trajectory cut off by epoch at 3 steps. Average EP return : 88.82222222222222 3997.0 45\n",
      "27 Warning: trajectory cut off by epoch at 64 steps. Average EP return : 83.74468085106383 3936.0 47\n",
      "28 Warning: trajectory cut off by epoch at 18 steps. Average EP return : 113.77142857142857 3982.0 35\n",
      "29 Warning: trajectory cut off by epoch at 48 steps. Average EP return : 96.39024390243902 3952.0 41\n",
      "30 Warning: trajectory cut off by epoch at 61 steps. Average EP return : 91.6046511627907 3939.0 43\n",
      "31 Warning: trajectory cut off by epoch at 48 steps. Average EP return : 96.39024390243902 3952.0 41\n",
      "32 Warning: trajectory cut off by epoch at 140 steps. Average EP return : 98.97435897435898 3860.0 39\n",
      "33 Warning: trajectory cut off by epoch at 73 steps. Average EP return : 98.175 3927.0 40\n",
      "34 Warning: trajectory cut off by epoch at 7 steps. Average EP return : 117.44117647058823 3993.0 34\n",
      "35 Warning: trajectory cut off by epoch at 11 steps. Average EP return : 104.97368421052632 3989.0 38\n",
      "36 Warning: trajectory cut off by epoch at 100 steps. Average EP return : 108.33333333333333 3900.0 36\n",
      "37 Warning: trajectory cut off by epoch at 82 steps. Average EP return : 97.95 3918.0 40\n",
      "38 Warning: trajectory cut off by epoch at 98 steps. Average EP return : 121.9375 3902.0 32\n",
      "39 Warning: trajectory cut off by epoch at 40 steps. Average EP return : 127.74193548387096 3960.0 31\n",
      "40 Warning: trajectory cut off by epoch at 105 steps. Average EP return : 139.10714285714286 3895.0 28\n",
      "41 Warning: trajectory cut off by epoch at 84 steps. Average EP return : 150.6153846153846 3916.0 26\n",
      "42 Warning: trajectory cut off by epoch at 17 steps. Average EP return : 173.17391304347825 3983.0 23\n",
      "43 Warning: trajectory cut off by epoch at 105 steps. Average EP return : 194.75 3895.0 20\n",
      "44 Warning: trajectory cut off by epoch at 97 steps. Average EP return : 195.15 3903.0 20\n",
      "45 Warning: trajectory cut off by epoch at 6 steps. Average EP return : 210.21052631578948 3994.0 19\n",
      "46 Warning: trajectory cut off by epoch at 23 steps. Average EP return : 165.70833333333334 3977.0 24\n",
      "47 Warning: trajectory cut off by epoch at 110 steps. Average EP return : 216.11111111111111 3890.0 18\n",
      "48 Warning: trajectory cut off by epoch at 151 steps. Average EP return : 213.83333333333334 3849.0 18\n",
      "49 Warning: trajectory cut off by epoch at 208 steps. Average EP return : 237.0 3792.0 16\n",
      "50 Warning: trajectory cut off by epoch at 4 steps. Average EP return : 249.75 3996.0 16\n",
      "51 Warning: trajectory cut off by epoch at 84 steps. Average EP return : 261.06666666666666 3916.0 15\n",
      "52 Warning: trajectory cut off by epoch at 230 steps. Average EP return : 290.0 3770.0 13\n",
      "53 Warning: trajectory cut off by epoch at 243 steps. Average EP return : 234.8125 3757.0 16\n",
      "54 Warning: trajectory cut off by epoch at 239 steps. Average EP return : 289.3076923076923 3761.0 13\n",
      "55 Warning: trajectory cut off by epoch at 222 steps. Average EP return : 290.61538461538464 3778.0 13\n",
      "56 Warning: trajectory cut off by epoch at 103 steps. Average EP return : 278.35714285714283 3897.0 14\n",
      "57 Warning: trajectory cut off by epoch at 169 steps. Average EP return : 319.25 3831.0 12\n",
      "58 Warning: trajectory cut off by epoch at 482 steps. Average EP return : 351.8 3518.0 10\n",
      "59 Warning: trajectory cut off by epoch at 303 steps. Average EP return : 410.77777777777777 3697.0 9\n",
      "60 Warning: trajectory cut off by epoch at 115 steps. Average EP return : 431.6666666666667 3885.0 9\n",
      "61 Warning: trajectory cut off by epoch at 51 steps. Average EP return : 438.77777777777777 3949.0 9\n",
      "62 Warning: trajectory cut off by epoch at 116 steps. Average EP return : 323.6666666666667 3884.0 12\n",
      "63 Warning: trajectory cut off by epoch at 234 steps. Average EP return : 289.6923076923077 3766.0 13\n",
      "64 Warning: trajectory cut off by epoch at 233 steps. Average EP return : 313.9166666666667 3767.0 12\n",
      "65 Warning: trajectory cut off by epoch at 118 steps. Average EP return : 388.2 3882.0 10\n",
      "66 Warning: trajectory cut off by epoch at 242 steps. Average EP return : 417.55555555555554 3758.0 9\n",
      "67 Warning: trajectory cut off by epoch at 73 steps. Average EP return : 357.0 3927.0 11\n",
      "68 Warning: trajectory cut off by epoch at 25 steps. Average EP return : 331.25 3975.0 12\n",
      "69 Warning: trajectory cut off by epoch at 112 steps. Average EP return : 353.45454545454544 3888.0 11\n",
      "70 Warning: trajectory cut off by epoch at 123 steps. Average EP return : 298.2307692307692 3877.0 13\n",
      "71 Warning: trajectory cut off by epoch at 176 steps. Average EP return : 273.14285714285717 3824.0 14\n",
      "72 Warning: trajectory cut off by epoch at 8 steps. Average EP return : 307.0769230769231 3992.0 13\n",
      "73 Warning: trajectory cut off by epoch at 131 steps. Average EP return : 322.4166666666667 3869.0 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 Warning: trajectory cut off by epoch at 9 steps. Average EP return : 362.8181818181818 3991.0 11\n",
      "75 Warning: trajectory cut off by epoch at 24 steps. Average EP return : 233.88235294117646 3976.0 17\n",
      "76 Warning: trajectory cut off by epoch at 124 steps. Average EP return : 352.3636363636364 3876.0 11\n",
      "77 Warning: trajectory cut off by epoch at 188 steps. Average EP return : 423.55555555555554 3812.0 9\n",
      "78 Warning: trajectory cut off by epoch at 86 steps. Average EP return : 489.25 3914.0 8\n",
      "79 Warning: trajectory cut off by epoch at 257 steps. Average EP return : 467.875 3743.0 8\n",
      "80 Warning: trajectory cut off by epoch at 75 steps. Average EP return : 490.625 3925.0 8\n",
      "81 Warning: trajectory cut off by epoch at 51 steps. Average EP return : 493.625 3949.0 8\n",
      "82 Warning: trajectory cut off by epoch at 329 steps. Average EP return : 458.875 3671.0 8\n",
      "83 Warning: trajectory cut off by epoch at 324 steps. Average EP return : 408.44444444444446 3676.0 9\n",
      "84 Warning: trajectory cut off by epoch at 290 steps. Average EP return : 371.0 3710.0 10\n",
      "85 Warning: trajectory cut off by epoch at 258 steps. Average EP return : 467.75 3742.0 8\n",
      "86 Warning: trajectory cut off by epoch at 183 steps. Average EP return : 424.1111111111111 3817.0 9\n",
      "87 Warning: trajectory cut off by epoch at 347 steps. Average EP return : 456.625 3653.0 8\n",
      "88 Warning: trajectory cut off by epoch at 38 steps. Average EP return : 396.2 3962.0 10\n",
      "89 Warning: trajectory cut off by epoch at 161 steps. Average EP return : 426.55555555555554 3839.0 9\n",
      "90 Warning: trajectory cut off by epoch at 2 steps. Average EP return : 444.22222222222223 3998.0 9\n",
      "91 Warning: trajectory cut off by epoch at 111 steps. Average EP return : 324.0833333333333 3889.0 12\n",
      "92 Warning: trajectory cut off by epoch at 388 steps. Average EP return : 451.5 3612.0 8\n",
      "93 Warning: trajectory cut off by epoch at 252 steps. Average EP return : 468.5 3748.0 8\n",
      "94 Warning: trajectory cut off by epoch at 217 steps. Average EP return : 472.875 3783.0 8\n",
      "95 Warning: trajectory cut off by epoch at 86 steps. Average EP return : 489.25 3914.0 8\n",
      "96 Warning: trajectory cut off by epoch at 438 steps. Average EP return : 445.25 3562.0 8\n",
      "97 Warning: trajectory cut off by epoch at 217 steps. Average EP return : 472.875 3783.0 8\n",
      "98 Warning: trajectory cut off by epoch at 70 steps. Average EP return : 491.25 3930.0 8\n",
      "99 Warning: trajectory cut off by epoch at 167 steps. Average EP return : 479.125 3833.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "101 Warning: trajectory cut off by epoch at 68 steps. Average EP return : 491.5 3932.0 8\n",
      "102 Warning: trajectory cut off by epoch at 7 steps. Average EP return : 499.125 3993.0 8\n",
      "103 Warning: trajectory cut off by epoch at 55 steps. Average EP return : 493.125 3945.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "105 Warning: trajectory cut off by epoch at 39 steps. Average EP return : 495.125 3961.0 8\n",
      "106 Warning: trajectory cut off by epoch at 118 steps. Average EP return : 431.3333333333333 3882.0 9\n",
      "107 Warning: trajectory cut off by epoch at 115 steps. Average EP return : 431.6666666666667 3885.0 9\n",
      "108 Warning: trajectory cut off by epoch at 172 steps. Average EP return : 425.3333333333333 3828.0 9\n",
      "109 Warning: trajectory cut off by epoch at 115 steps. Average EP return : 485.625 3885.0 8\n",
      "110 Warning: trajectory cut off by epoch at 325 steps. Average EP return : 459.375 3675.0 8\n",
      "111 Warning: trajectory cut off by epoch at 211 steps. Average EP return : 378.9 3789.0 10\n",
      "112 Warning: trajectory cut off by epoch at 273 steps. Average EP return : 310.5833333333333 3727.0 12\n",
      "113 Warning: trajectory cut off by epoch at 93 steps. Average EP return : 300.53846153846155 3907.0 13\n",
      "114 Warning: trajectory cut off by epoch at 33 steps. Average EP return : 330.5833333333333 3967.0 12\n",
      "115 Warning: trajectory cut off by epoch at 210 steps. Average EP return : 473.75 3790.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "117 Warning: trajectory cut off by epoch at 281 steps. Average EP return : 464.875 3719.0 8\n",
      "118 Warning: trajectory cut off by epoch at 57 steps. Average EP return : 492.875 3943.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "122 Warning: trajectory cut off by epoch at 193 steps. Average EP return : 475.875 3807.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "125 Warning: trajectory cut off by epoch at 303 steps. Average EP return : 462.125 3697.0 8\n",
      "126 Warning: trajectory cut off by epoch at 307 steps. Average EP return : 461.625 3693.0 8\n",
      "127 Warning: trajectory cut off by epoch at 72 steps. Average EP return : 436.44444444444446 3928.0 9\n",
      " Average EP return : 500.0 4000.0 8\n",
      "129 Warning: trajectory cut off by epoch at 11 steps. Average EP return : 498.625 3989.0 8\n",
      "130 Warning: trajectory cut off by epoch at 41 steps. Average EP return : 439.8888888888889 3959.0 9\n",
      " Average EP return : 500.0 4000.0 8\n",
      "132 Warning: trajectory cut off by epoch at 70 steps. Average EP return : 491.25 3930.0 8\n",
      "133 Warning: trajectory cut off by epoch at 48 steps. Average EP return : 494.0 3952.0 8\n",
      "134 Warning: trajectory cut off by epoch at 95 steps. Average EP return : 488.125 3905.0 8\n",
      "135 Warning: trajectory cut off by epoch at 74 steps. Average EP return : 490.75 3926.0 8\n",
      "136 Warning: trajectory cut off by epoch at 268 steps. Average EP return : 466.5 3732.0 8\n",
      "137 Warning: trajectory cut off by epoch at 150 steps. Average EP return : 481.25 3850.0 8\n",
      "138 Warning: trajectory cut off by epoch at 481 steps. Average EP return : 439.875 3519.0 8\n",
      "139 Warning: trajectory cut off by epoch at 165 steps. Average EP return : 479.375 3835.0 8\n",
      "140 Warning: trajectory cut off by epoch at 9 steps. Average EP return : 498.875 3991.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "142 Warning: trajectory cut off by epoch at 157 steps. Average EP return : 480.375 3843.0 8\n",
      "143 Warning: trajectory cut off by epoch at 290 steps. Average EP return : 463.75 3710.0 8\n",
      "144 Warning: trajectory cut off by epoch at 449 steps. Average EP return : 443.875 3551.0 8\n",
      "145 Warning: trajectory cut off by epoch at 274 steps. Average EP return : 465.75 3726.0 8\n",
      "146 Warning: trajectory cut off by epoch at 190 steps. Average EP return : 317.5 3810.0 12\n",
      "147 Warning: trajectory cut off by epoch at 117 steps. Average EP return : 388.3 3883.0 10\n",
      "148 Warning: trajectory cut off by epoch at 159 steps. Average EP return : 349.1818181818182 3841.0 11\n",
      "149 Warning: trajectory cut off by epoch at 236 steps. Average EP return : 376.4 3764.0 10\n",
      "150 Warning: trajectory cut off by epoch at 23 steps. Average EP return : 397.7 3977.0 10\n",
      "151 Warning: trajectory cut off by epoch at 64 steps. Average EP return : 437.3333333333333 3936.0 9\n",
      "152 Warning: trajectory cut off by epoch at 189 steps. Average EP return : 476.375 3811.0 8\n",
      "153 Warning: trajectory cut off by epoch at 101 steps. Average EP return : 487.375 3899.0 8\n",
      "154 Warning: trajectory cut off by epoch at 236 steps. Average EP return : 470.5 3764.0 8\n",
      "155 Warning: trajectory cut off by epoch at 339 steps. Average EP return : 406.77777777777777 3661.0 9\n",
      "156 Warning: trajectory cut off by epoch at 481 steps. Average EP return : 391.0 3519.0 9\n",
      "157 Warning: trajectory cut off by epoch at 244 steps. Average EP return : 469.5 3756.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "161 Warning: trajectory cut off by epoch at 325 steps. Average EP return : 408.3333333333333 3675.0 9\n",
      "162 Warning: trajectory cut off by epoch at 329 steps. Average EP return : 458.875 3671.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "164 Warning: trajectory cut off by epoch at 2 steps. Average EP return : 444.22222222222223 3998.0 9\n",
      "165 Warning: trajectory cut off by epoch at 327 steps. Average EP return : 408.1111111111111 3673.0 9\n",
      "166 Warning: trajectory cut off by epoch at 43 steps. Average EP return : 439.6666666666667 3957.0 9\n",
      "167 Warning: trajectory cut off by epoch at 258 steps. Average EP return : 467.75 3742.0 8\n",
      "168 Warning: trajectory cut off by epoch at 11 steps. Average EP return : 498.625 3989.0 8\n",
      "169 Warning: trajectory cut off by epoch at 65 steps. Average EP return : 491.875 3935.0 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170 Warning: trajectory cut off by epoch at 48 steps. Average EP return : 494.0 3952.0 8\n",
      "171 Warning: trajectory cut off by epoch at 113 steps. Average EP return : 485.875 3887.0 8\n",
      "172 Warning: trajectory cut off by epoch at 208 steps. Average EP return : 474.0 3792.0 8\n",
      "173 Warning: trajectory cut off by epoch at 2 steps. Average EP return : 499.75 3998.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "176 Warning: trajectory cut off by epoch at 290 steps. Average EP return : 463.75 3710.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "178 Warning: trajectory cut off by epoch at 30 steps. Average EP return : 441.1111111111111 3970.0 9\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "190 Warning: trajectory cut off by epoch at 315 steps. Average EP return : 460.625 3685.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "193 Warning: trajectory cut off by epoch at 71 steps. Average EP return : 491.125 3929.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "195 Warning: trajectory cut off by epoch at 213 steps. Average EP return : 473.375 3787.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "198 Warning: trajectory cut off by epoch at 13 steps. Average EP return : 398.7 3987.0 10\n",
      "199 Warning: trajectory cut off by epoch at 12 steps. Average EP return : 306.7692307692308 3988.0 13\n",
      "200 Warning: trajectory cut off by epoch at 457 steps. Average EP return : 442.875 3543.0 8\n",
      "201 Warning: trajectory cut off by epoch at 314 steps. Average EP return : 460.75 3686.0 8\n",
      "202 Warning: trajectory cut off by epoch at 60 steps. Average EP return : 492.5 3940.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "204 Warning: trajectory cut off by epoch at 17 steps. Average EP return : 442.55555555555554 3983.0 9\n",
      "205 Warning: trajectory cut off by epoch at 14 steps. Average EP return : 398.6 3986.0 10\n",
      "206 Warning: trajectory cut off by epoch at 346 steps. Average EP return : 456.75 3654.0 8\n",
      "207 Warning: trajectory cut off by epoch at 97 steps. Average EP return : 487.875 3903.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "210 Warning: trajectory cut off by epoch at 66 steps. Average EP return : 491.75 3934.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "217 Warning: trajectory cut off by epoch at 26 steps. Average EP return : 441.55555555555554 3974.0 9\n",
      "218 Warning: trajectory cut off by epoch at 17 steps. Average EP return : 398.3 3983.0 10\n",
      "219 Warning: trajectory cut off by epoch at 327 steps. Average EP return : 367.3 3673.0 10\n",
      "220 Warning: trajectory cut off by epoch at 361 steps. Average EP return : 363.9 3639.0 10\n",
      "221 Warning: trajectory cut off by epoch at 94 steps. Average EP return : 434.0 3906.0 9\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "226 Warning: trajectory cut off by epoch at 127 steps. Average EP return : 484.125 3873.0 8\n",
      "227 Warning: trajectory cut off by epoch at 386 steps. Average EP return : 451.75 3614.0 8\n",
      "228 Warning: trajectory cut off by epoch at 227 steps. Average EP return : 419.22222222222223 3773.0 9\n",
      "229 Warning: trajectory cut off by epoch at 63 steps. Average EP return : 492.125 3937.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "231 Warning: trajectory cut off by epoch at 181 steps. Average EP return : 477.375 3819.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "234 Warning: trajectory cut off by epoch at 392 steps. Average EP return : 451.0 3608.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "236 Warning: trajectory cut off by epoch at 47 steps. Average EP return : 494.125 3953.0 8\n",
      "237 Warning: trajectory cut off by epoch at 5 steps. Average EP return : 499.375 3995.0 8\n",
      "238 Warning: trajectory cut off by epoch at 278 steps. Average EP return : 465.25 3722.0 8\n",
      "239 Warning: trajectory cut off by epoch at 22 steps. Average EP return : 442.0 3978.0 9\n",
      "240 Warning: trajectory cut off by epoch at 124 steps. Average EP return : 430.6666666666667 3876.0 9\n",
      "241 Warning: trajectory cut off by epoch at 495 steps. Average EP return : 350.5 3505.0 10\n",
      "242 Warning: trajectory cut off by epoch at 44 steps. Average EP return : 329.6666666666667 3956.0 12\n",
      "243 Warning: trajectory cut off by epoch at 57 steps. Average EP return : 438.1111111111111 3943.0 9\n",
      "244 Warning: trajectory cut off by epoch at 28 steps. Average EP return : 361.09090909090907 3972.0 11\n",
      "245 Warning: trajectory cut off by epoch at 338 steps. Average EP return : 406.8888888888889 3662.0 9\n",
      "246 Warning: trajectory cut off by epoch at 465 steps. Average EP return : 441.875 3535.0 8\n",
      "247 Warning: trajectory cut off by epoch at 123 steps. Average EP return : 484.625 3877.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      " Average EP return : 500.0 4000.0 8\n",
      "Evaluating PPO... Epochs: 100   Steps per Epoch: 4000\n",
      "0 ppo ep ret 500.0 500\n",
      "Average ppo ep ret for  100 500.0\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--conf_file', type=str, default='config_NCartPole_1.yaml')\n",
    "args = parser.parse_args(\"\")\n",
    "print(\"Config_File_Name : \", args.conf_file)\n",
    "\n",
    "#Reading Experiment Configurations\n",
    "config_yaml = read_config(args.conf_file)\n",
    "config = Configs(config_yaml)\n",
    "ppo_act_fn = eval(config.ppo_act_fn)\n",
    "\n",
    "\n",
    "\n",
    "#Get the execution device\n",
    "run_count = 1\n",
    "run_dir = \"\"\n",
    "ppo_dir = \"\"\n",
    "exp_name = \"\"\n",
    "\n",
    "seed = config.aa_exp_seeds[0]\n",
    "ppo_hid_nodes = config.ppo_hid_nodes[0]\n",
    "env_name = config.env_name[0]\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "overwrite_flag = True\n",
    "\n",
    "kwargs={\"ncart\":4}\n",
    "env = gym.make(env_name, **kwargs)\n",
    "obs_dim = env.env.observation_space.shape\n",
    "act_dim = env.env.action_space.shape\n",
    "\n",
    "ppo_dir = \"runs/\" + str(env_name) + \"/ppo_\" + str(ppo_hid_nodes) + \"/seed_\" + str(seed)\n",
    "create_dir(ppo_dir)\n",
    "\n",
    "device = get_exec_device()\n",
    "\n",
    "#Original PPO\n",
    "org_ppo, ppo_weights, ppo_biases, ppo_std = fn_train_ppo(env, ppo_hid_nodes, seed, ppo_dir, ppo_act_fn, config.ppo_epochs, config.ppo_epoch_steps, config.ppo_gamma)\n",
    "fn_evaluate_ppo(env, org_ppo, ppo_dir, config.evl_epochs, config.evl_max_epoch_steps, None, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
