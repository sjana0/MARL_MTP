{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a930cb5",
   "metadata": {},
   "source": [
    "Import All Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "809a94bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import yaml, argparse, os, json\n",
    "#from multiprocessing import Process\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "from scipy import signal\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Box, Discrete\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6b1d83",
   "metadata": {},
   "source": [
    "Set Run Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40d3e5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_count = 0\n",
    "tb = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d0edbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# env = gym.make(\"CartPole-v1\")\n",
    "# env_obs_size = env.observation_space.shape[0]\n",
    "# env_obs_size\n",
    "# dummy_input = torch.randn(1, env_obs_size, requires_grad=True)\n",
    "# dummy_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b889330a",
   "metadata": {},
   "source": [
    "Set Run Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0547c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 0\n",
    "# np.random.seed(seed)\n",
    "# random.seed(seed)\n",
    "# torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c28a6",
   "metadata": {},
   "source": [
    "Torch Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c76afb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegDataset(Dataset):\n",
    "    def __init__(self, df_x, df_y, device=\"cpu\", seed=0): \n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        self.seed = seed\n",
    "        self.device=device\n",
    "        x=df_x.values\n",
    "        y=df_y.values\n",
    "        self.x_train=torch.tensor(x,dtype=torch.float32).to(device=device)\n",
    "        self.y_train=torch.tensor(y,dtype=torch.float32).to(device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.x_train[idx],self.y_train[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1525f687",
   "metadata": {},
   "source": [
    "Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "348e77bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length,)\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def count_vars(module):\n",
    "    return sum([np.prod(p.shape) for p in module.parameters()])\n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    return signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
    "\n",
    "def statistics_scalar(x, with_min_and_max=False):\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    global_sum, global_n = (np.sum(x), len(x))\n",
    "    mean = global_sum / global_n\n",
    "\n",
    "    global_sum_sq = np.sum((x - mean)**2)\n",
    "    std = np.sqrt(global_sum_sq / global_n)  # compute global std\n",
    "\n",
    "    if with_min_and_max:\n",
    "        global_min = np.min(x) if len(x) > 0 else np.inf\n",
    "        global_max = np.max(x) if len(x) > 0 else -np.inf\n",
    "        return mean, std, global_min, global_max\n",
    "    return mean, std\n",
    "\n",
    "class Configs(object):\n",
    "    def __init__(self, confg):\n",
    "        for key in confg:\n",
    "            setattr(self, key, confg[key])\n",
    "\n",
    "def get_exec_device():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using device:', device)\n",
    "    return device\n",
    "\n",
    "def calc_accuracy(lst_y, lst_y_h):\n",
    "    #print(lst_y, lst_y_h)\n",
    "    total_count = len(lst_y)\n",
    "    total_correct = 0\n",
    "    for y, y_h in zip(lst_y, lst_y_h):\n",
    "        #print(y, y_h)\n",
    "        if y == y_h:\n",
    "            total_correct += 1\n",
    "    return float(total_correct/total_count)\n",
    "\n",
    "def calc_mse(lst_y, lst_y_h):\n",
    "    np_lst_y = np.array(lst_y).flatten()\n",
    "    np_lst_y_h = np.array(lst_y_h).flatten()\n",
    "    return np.sum((np_lst_y - np_lst_y_h)**2)/len(np_lst_y_h)\n",
    "\n",
    "\n",
    "def evaluate_ppo(env, ppo, evl_epochs, evl_max_epoch_steps, device=\"cpu\"):\n",
    "    lst_ep_ret = []\n",
    "    lst_ep_len = []\n",
    "    for epoch in range(evl_epochs):\n",
    "        #print(epoch)\n",
    "        o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "        for t in range(evl_max_epoch_steps):\n",
    "            a_h = ppo.ac.evaluate(torch.as_tensor(o, dtype=torch.float32).to(device=device))#rule_list.get_arg_max([o])[0]\n",
    "            #print(ep_ret, \">>\", a_h)\n",
    "            print(\"blashdhsdhasdasdasdhasdnada action: \", a_h)\n",
    "            next_o, r, d, _ = env.step(a_h)\n",
    "            o = next_o\n",
    "            terminal = d\n",
    "            ep_ret += float(r)\n",
    "            ep_len += 1\n",
    "            if terminal:\n",
    "                break\n",
    "        lst_ep_ret.append(ep_ret)\n",
    "        lst_ep_len.append(ep_len)\n",
    "        if epoch%100==0:\n",
    "            print(epoch, \"ppo ep ret\", ep_ret, ep_len)\n",
    "        o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "    print(\"Average ppo ep ret for \", evl_epochs, sum(lst_ep_ret)/len(lst_ep_ret))\n",
    "    return lst_ep_ret, lst_ep_len\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_list_as_csv(lst_x, path, name, columns=None):\n",
    "    #np.savetxt(path+\"/\"+name, np.array(lst_x), delimiter =\", \", fmt ='% s')\n",
    "    pd.DataFrame(np.transpose(np.array(lst_x)), columns=columns).to_csv(path_or_buf=path+\"/\"+name, sep=',')\n",
    "\n",
    "def write_to_tensorboard(lst_x, lst_y, name):\n",
    "    pass\n",
    "\n",
    "def read_config(file_name):\n",
    "    with open(file_name) as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    return config\n",
    "\n",
    "def update_config(config, file_name):\n",
    "    with open(file_name, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    gym.utils.seeding.np_random(seed)\n",
    "\n",
    "def create_dir(dir_name):\n",
    "    success=True\n",
    "    try:\n",
    "        os.makedirs(dir_name)\n",
    "    except:\n",
    "        success=False\n",
    "    return success\n",
    "\n",
    "def get_env_action_space_type(action_space):\n",
    "    if isinstance(action_space, Box):\n",
    "        return \"box\"\n",
    "    elif isinstance(action_space, Discrete):\n",
    "        return \"discrete\"\n",
    "\n",
    "def get_env_dims(env):\n",
    "    env_obs_size = 0\n",
    "    env_act_size = 0\n",
    "    env_obs_size = env.observation_space.shape[0]\n",
    "    if get_env_action_space_type(env.action_space)==\"box\":\n",
    "        env_act_size = env.action_space.shape[0]\n",
    "    elif get_env_action_space_type(env.action_space)==\"discrete\":\n",
    "        env_act_size = env.action_space.n\n",
    "    return env_obs_size, env_act_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7513a53e",
   "metadata": {},
   "source": [
    "asdasd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb9065b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.nn_layers = nn.ModuleList()\n",
    "        self.nn_layers_act = []\n",
    "        self.x_m_1 = None\n",
    "        self.x_w_r = None\n",
    "        self.layers_size = len(layers)\n",
    "        \n",
    "        for i in range(0, len(layers), 2):\n",
    "            ly = layers[i]\n",
    "            act = layers[i+1]\n",
    "            self.nn_layers.append(ly)\n",
    "            self.nn_layers.append(act)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_m_1 = None        \n",
    "        #for ly, act in zip(self.nn_layers, self.nn_layers_act):\n",
    "        for i in range(0, self.layers_size, 2):\n",
    "            x_m_1 = x\n",
    "            ly = self.nn_layers[i]\n",
    "            act = self.nn_layers[i+1]\n",
    "            x = ly(x)\n",
    "            self.x_w_r = x\n",
    "            x = act(x)\n",
    "        self.x_m_1 = x_m_1\n",
    "        return x\n",
    "\n",
    "\n",
    "def construct_mlp(sizes, activation, output_activation=nn.Identity, device=\"cpu\"):\n",
    "    layers = []\n",
    "    print(\"MLP_Sizes\", sizes)\n",
    "    for j in range(len(sizes)-1):\n",
    "        act = activation if j < len(sizes)-2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    m__ = MLP(layers).to(device=device)\n",
    "    print(\"NN Model:\", m__, activation, output_activation)\n",
    "    return  m__\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def _distribution(self, obs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, obs, act=None):\n",
    "        pi = self._distribution(obs)\n",
    "        logp_a = None\n",
    "        if act is not None:\n",
    "            logp_a = self._log_prob_from_distribution(pi, act)\n",
    "        return pi, logp_a\n",
    "\n",
    "\n",
    "class MLPCategoricalActor(Actor):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.logits_net = construct_mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation, device=device)\n",
    "        self.nn_net = self.logits_net\n",
    "\n",
    "    def _distribution(self, obs):\n",
    "        logits = self.logits_net(obs)\n",
    "        return Categorical(logits=logits)\n",
    "    \n",
    "    def arg_max(self, obs):\n",
    "        logits = self.logits_net(obs)\n",
    "        return logits.argmax(dim=-1)\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        return pi.log_prob(act)\n",
    "\n",
    "\n",
    "class MLPGaussianActor(Actor):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        log_std = -0.5 * np.ones(act_dim, dtype=np.float32)\n",
    "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n",
    "        self.mu_net = construct_mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation, device=device)\n",
    "        self.nn_net = self.mu_net\n",
    "\n",
    "    def _distribution(self, obs):\n",
    "        mu = self.mu_net(obs)\n",
    "        std = torch.exp(self.log_std.to(device=device))\n",
    "        return Normal(mu, std)\n",
    "    \n",
    "    def arg_max(self, obs):\n",
    "        mu = self.mu_net(obs)\n",
    "        #std = torch.exp(self.log_std)\n",
    "        return mu#.detach().cpu().numpy()#Normal(mu, std).sample()\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        return pi.log_prob(act).sum(axis=-1)    # Last axis sum needed for Torch Normal distribution\n",
    "\n",
    "\n",
    "class MLPCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, hidden_sizes, activation, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.v_net = construct_mlp([obs_dim] + list(hidden_sizes) + [1], activation, device=device)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return torch.squeeze(self.v_net(obs), -1) # Critical to ensure v has right shape.\n",
    "\n",
    "\n",
    "class MLPActorCritic(nn.Module):\n",
    "    def __init__(self, observation_space, action_space, device=\"cpu\", hidden_sizes=[64,64], activation=nn.ReLU, seed=100):\n",
    "        super().__init__()\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        self.seed = seed\n",
    "        obs_dim = observation_space.shape[0]\n",
    "\n",
    "        # policy builder depends on action space\n",
    "        if get_env_action_space_type(action_space)==\"box\":\n",
    "            self.pi = MLPGaussianActor(obs_dim, action_space.shape[0], hidden_sizes, activation, device=device)\n",
    "        elif get_env_action_space_type(action_space)==\"discrete\":\n",
    "            self.pi = MLPCategoricalActor(obs_dim, action_space.n, hidden_sizes, activation, device=device)\n",
    "\n",
    "        # build value function\n",
    "        self.v  = MLPCritic(obs_dim, hidden_sizes, activation, device=device)\n",
    "\n",
    "    def step(self, obs):\n",
    "        with torch.no_grad():\n",
    "            pi = self.pi._distribution(obs)\n",
    "            a = pi.sample()\n",
    "            logp_a = self.pi._log_prob_from_distribution(pi, a)\n",
    "            v = self.v(obs)\n",
    "        return a.cpu().numpy(), v.cpu().numpy(), logp_a.cpu().numpy()\n",
    "    \n",
    "    def get_prev_layer_val(self):\n",
    "        return self.pi.nn_net.x_m_1\n",
    "    \n",
    "    def evaluate(self, obs):\n",
    "        with torch.no_grad():\n",
    "            a = self.pi.arg_max(obs)\n",
    "        return a.cpu().numpy()\n",
    "\n",
    "    def act(self, obs):\n",
    "        return self.step(obs)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e622c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOBuffer:\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95, device=\"cpu\"):\n",
    "        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "        self.device=device\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "        \n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "        adv_mean, adv_std = statistics_scalar(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
    "                    adv=self.adv_buf, logp=self.logp_buf)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32).to(self.device) for k,v in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a5b2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO():\n",
    "    def __init__(self, env, actor_critic=MLPActorCritic, ac_kwargs=dict(), seed=0, \n",
    "            steps_per_epoch=4000, epochs=50, gamma=0.99, clip_ratio=0.2, pi_lr=3e-4,\n",
    "            vf_lr=1e-3, train_pi_iters=80, train_v_iters=80, lam=0.97, max_ep_len=1000,\n",
    "            target_kl=0.01, logger_kwargs=dict(), save_freq=10, exp_name=\".\", exp_dir=\".\", device=\"cpu\", act_fn=nn.ReLU, tb=None):\n",
    "        self.lst_avg_episode_ret = []\n",
    "        self.lst_test_traj = []\n",
    "        self.env = env\n",
    "        self.actor_critic = actor_critic\n",
    "        self.ac_kwargs = ac_kwargs\n",
    "        self.seed = seed\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.epochs = epochs\n",
    "        self.gamma = gamma\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.pi_lr = pi_lr\n",
    "        self.vf_lr = vf_lr\n",
    "        self.train_pi_iters = train_pi_iters\n",
    "        self.train_v_iters = train_v_iters\n",
    "        self.lam = lam\n",
    "        self.max_ep_len = max_ep_len\n",
    "        self.target_kl = target_kl\n",
    "        self.logger_kwargs = logger_kwargs\n",
    "        self.save_freq = save_freq\n",
    "        self.exp_name = exp_name\n",
    "        self.exp_dir = exp_dir\n",
    "        self.device = device\n",
    "        self.act_fn = act_fn\n",
    "        self.tb = tb #TBWriter(exp_dir+\"/ppo\")\n",
    "        self.lst_train_obs = []\n",
    "\n",
    "        # Random seed\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        # Instantiate environment\n",
    "        #self.env = self.env_fn()\n",
    "        self.env.seed(seed)\n",
    "        self.obs_dim = self.env.observation_space.shape\n",
    "        self.act_dim = self.env.action_space.shape\n",
    "        #print(\">>>>>>>>>>>>>>>>\", self.obs_dim, self.act_dim)\n",
    "\n",
    "        # Create actor-critic module\n",
    "        self.ac = self.actor_critic(self.env.observation_space, self.env.action_space, self.device, activation=self.act_fn, **ac_kwargs)\n",
    "\n",
    "        # Count variables\n",
    "        self.var_counts = tuple(count_vars(module) for module in [self.ac.pi, self.ac.v])\n",
    "\n",
    "        # Set up experience buffer\n",
    "        self.local_steps_per_epoch = self.steps_per_epoch #int(steps_per_epoch / num_procs())\n",
    "        self.buf = PPOBuffer(self.obs_dim, self.act_dim, self.local_steps_per_epoch, self.gamma, self.lam, device=self.device)\n",
    "\n",
    "        # Set up optimizers for policy and value function\n",
    "        self.pi_optimizer = Adam(self.ac.pi.parameters(), lr=pi_lr)\n",
    "        self.vf_optimizer = Adam(self.ac.v.parameters(), lr=vf_lr)\n",
    "\n",
    "    # Set up function for computing PPO policy loss\n",
    "    def compute_loss_pi(self, data):\n",
    "        obs, act, adv, logp_old = data['obs'], data['act'], data['adv'], data['logp']\n",
    "\n",
    "        # Policy loss\n",
    "        pi, logp = self.ac.pi(obs, act)\n",
    "        ratio = torch.exp(logp - logp_old)\n",
    "        clip_adv = torch.clamp(ratio, 1-self.clip_ratio, 1+self.clip_ratio) * adv\n",
    "        loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()\n",
    "\n",
    "        # Useful extra info\n",
    "        approx_kl = (logp_old - logp).mean().item()\n",
    "        ent = pi.entropy().mean().item()\n",
    "        clipped = ratio.gt(1+self.clip_ratio) | ratio.lt(1-self.clip_ratio)\n",
    "        clipfrac = torch.as_tensor(clipped, dtype=torch.float32).mean().item()\n",
    "        pi_info = dict(kl=approx_kl, ent=ent, cf=clipfrac)\n",
    "\n",
    "        return loss_pi, pi_info\n",
    "\n",
    "    # Set up function for computing value loss\n",
    "    def compute_loss_v(self, data):\n",
    "        obs, ret = data['obs'], data['ret']\n",
    "        return ((self.ac.v(obs) - ret)**2).mean()\n",
    "\n",
    "    def update(self, epoch):\n",
    "        data = self.buf.get()\n",
    "\n",
    "        pi_l_old, pi_info_old = self.compute_loss_pi(data)\n",
    "        pi_l_old = pi_l_old.item()\n",
    "        v_l_old = self.compute_loss_v(data).item()\n",
    "\n",
    "        pi_upd_iter = 0\n",
    "        total_loss_pi = 0.0\n",
    "        v_upd_iter = 0\n",
    "        total_loss_v = 0.0\n",
    "        # Train policy with multiple steps of gradient descent\n",
    "        for i in range(self.train_pi_iters):\n",
    "            pi_upd_iter += 1\n",
    "            self.pi_optimizer.zero_grad()\n",
    "            loss_pi, pi_info = self.compute_loss_pi(data)\n",
    "            total_loss_pi += loss_pi.item()\n",
    "            kl = pi_info['kl']\n",
    "            if kl > 1.5 * self.target_kl:\n",
    "                #logger.log('Early stopping at step %d due to reaching max kl.'%i)\n",
    "                break\n",
    "            loss_pi.backward()   # average grads across MPI processes\n",
    "            self.pi_optimizer.step()\n",
    "\n",
    "        # Value function learning\n",
    "        for i in range(self.train_v_iters):\n",
    "            v_upd_iter += 1\n",
    "            self.vf_optimizer.zero_grad()\n",
    "            loss_v = self.compute_loss_v(data)\n",
    "            total_loss_v += loss_v.item()\n",
    "            loss_v.backward()    # average grads across MPI processes\n",
    "            self.vf_optimizer.step()\n",
    "        \n",
    "        if self.tb is not None:\n",
    "            self.tb.writer.add_scalar('Train/KL', pi_info['kl'], epoch)    \n",
    "            self.tb.writer.add_scalar(\"Train/ENT\", pi_info_old['ent'], epoch)\n",
    "            self.tb.writer.add_scalar(\"Train/CF\", pi_info['cf'], epoch)\n",
    "            self.tb.writer.add_scalar(\"Train/pi_l_old\", pi_l_old, epoch)\n",
    "            self.tb.writer.add_scalar(\"Train/v_l_old\", v_l_old, epoch)\n",
    "            self.tb.writer.add_scalar(\"Train/pi_l_avg\", total_loss_pi/pi_upd_iter, epoch)\n",
    "            self.tb.writer.add_scalar(\"Train/v_l_avg\", total_loss_v/v_upd_iter, epoch)\n",
    "        # Log changes from update\n",
    "        kl, ent, cf = pi_info['kl'], pi_info_old['ent'], pi_info['cf']\n",
    "\n",
    "\n",
    "    def ppo_train(self):\n",
    "        #start_time = time.time()\n",
    "        o, ep_ret, ep_len = self.env.reset(), 0, 0\n",
    "\n",
    "        # Main loop: collect experience in env and update/log each epoch\n",
    "        lst_epochs_avg_ep_ret = []\n",
    "        episodes = 0\n",
    "        for epoch in range(self.epochs):\n",
    "            total_epoch_ret = 0.0\n",
    "            epoch_ep_count = 0\n",
    "            for t in range(self.local_steps_per_epoch):\n",
    "                self.lst_train_obs.append(o)\n",
    "                a, v, logp = self.ac.step(torch.as_tensor(o, dtype=torch.float32).to(device=self.device))\n",
    "                next_o, r, d, _ = self.env.step(a)\n",
    "                ep_ret += r\n",
    "                ep_len += 1\n",
    "\n",
    "                # save and log\n",
    "                self.buf.store(o, a, r, v, logp)\n",
    "                \n",
    "                # Update obs (critical!)\n",
    "                o = next_o\n",
    "\n",
    "                timeout = ep_len == self.max_ep_len\n",
    "                terminal = d or timeout\n",
    "                epoch_ended = t==self.local_steps_per_epoch-1\n",
    "\n",
    "                if terminal or epoch_ended:\n",
    "                    if epoch_ended and not(terminal):\n",
    "                        print(epoch, 'Warning: trajectory cut off by epoch at %d steps.'%ep_len, flush=True, end=\"\")\n",
    "                    else:\n",
    "                        total_epoch_ret += ep_ret\n",
    "                        epoch_ep_count += 1\n",
    "                        if self.tb is not None:\n",
    "                            self.tb.writer.add_scalar('Train/Episode_Return', ep_ret, episodes)\n",
    "                            self.tb.writer.add_scalar('Train/Episode_Length', ep_len, episodes)\n",
    "                        episodes += 1\n",
    "                    # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                    if timeout or epoch_ended:\n",
    "                        _, v, _ = self.ac.step(torch.as_tensor(o, dtype=torch.float32).to(device=self.device))\n",
    "                    else:\n",
    "                        v = 0\n",
    "                    self.buf.finish_path(v)\n",
    "                    o, ep_ret, ep_len = self.env.reset(), 0, 0\n",
    "\n",
    "            # Perform PPO update!\n",
    "            self.update(epoch)\n",
    "\n",
    "            if epoch_ep_count == 0:\n",
    "                epoch_ep_count = 1\n",
    "            lst_epochs_avg_ep_ret.append(total_epoch_ret/epoch_ep_count)\n",
    "            print(\" Average EP return :\", total_epoch_ret/epoch_ep_count, total_epoch_ret, epoch_ep_count)\n",
    "            if self.tb is not None:\n",
    "                self.tb.writer.add_scalar('Train/Epoch_Return_Avg', total_epoch_ret/epoch_ep_count, epoch)    \n",
    "                self.tb.writer.add_histogram(\"Train/pi_model_weight_last_layer\", self.get_ppo_actor_weights_std()[0], epoch)\n",
    "                self.tb.writer.add_histogram(\"Train/pi_model_bias_last_layer\", self.get_ppo_actor_weights_std()[1], epoch)\n",
    "            if epoch%self.save_freq == 0:\n",
    "                self.save_ppo_model()        \n",
    "        self.save_ppo_model_final()\n",
    "        np.savetxt(self.exp_dir+\"/\"+\"lst_ppo_train_obs.csv\", np.array(self.lst_train_obs), delimiter =\", \", fmt ='% s')\n",
    "        return lst_epochs_avg_ep_ret\n",
    "            \n",
    "        #def ppo_test():\n",
    "    def save_ppo_model(self):\n",
    "        torch.save(self.ac.pi.state_dict(), self.exp_dir+\"/\"+\"ac_pi.mld\")\n",
    "        torch.save(self.ac.v.state_dict(), self.exp_dir+\"/\"+\"ac_v.mld\")\n",
    "    \n",
    "    def save_ppo_model_final(self):\n",
    "        torch.save(self.ac.pi.state_dict(), self.exp_dir+\"/\"+\"ac_pi.mld\")\n",
    "        torch.save(self.ac.v.state_dict(), self.exp_dir+\"/\"+\"ac_v.mld\")\n",
    "\n",
    "    def load_ppo_model(self):\n",
    "        self.ac.pi.load_state_dict(torch.load(self.exp_dir+\"/\"+\"ac_pi.mld\"))\n",
    "        self.ac.v.load_state_dict(torch.load(self.exp_dir+\"/\"+\"ac_v.mld\"))\n",
    "    \n",
    "    def get_ppo_actor_weights_std(self, layer_indx=-2):\n",
    "        weights = self.ac.pi.nn_net.nn_layers[layer_indx].weight.detach().cpu().numpy()\n",
    "        bias = self.ac.pi.nn_net.nn_layers[layer_indx].bias.detach().cpu().numpy()\n",
    "        if get_env_action_space_type(env.action_space)==\"box\":\n",
    "            std = torch.exp(self.ac.pi.log_std).detach().cpu().numpy()\n",
    "        else:\n",
    "            std = None\n",
    "        return weights, bias, std\n",
    "\n",
    "    def ppo_evaluation_dataset(self, epochs_count=0, max_epoch_steps=0):\n",
    "        print(\"Generating PPO Evaluation Dataset.....\")\n",
    "        lst_test_trajs = []\n",
    "        if os.path.exists(self.exp_dir+\"/\"+\"lst_ppo_train_obs.csv\"):\n",
    "            self.lst_train_obs = np.loadtxt(self.exp_dir+\"/\"+\"lst_ppo_train_obs.csv\", delimiter=\",\", dtype=float)\n",
    "            for o in (self.lst_train_obs):\n",
    "                a = self.ac.evaluate(torch.as_tensor(o, dtype=torch.float32).to(device=self.device))\n",
    "                prev_layer_val = self.ac.get_prev_layer_val().cpu().detach().cpu().numpy()\n",
    "                if get_env_action_space_type(self.env.action_space) == \"box\":\n",
    "                    lst_test_trajs.append(np.array(o.tolist()+prev_layer_val.tolist()+a.tolist()+[-1, -1, -1],dtype=float))\n",
    "                else:\n",
    "                    lst_test_trajs.append(np.array(o.tolist()+prev_layer_val.tolist()+[a[()]]+[-1, -1, -1],dtype=float))\n",
    "        \n",
    "        o, ep_ret, ep_len = self.env.reset(), 0, 0\n",
    "        for epoch in range(epochs_count):\n",
    "            for t in range(max_epoch_steps):\n",
    "                a = self.ac.evaluate(torch.as_tensor(o, dtype=torch.float32).to(device=self.device))\n",
    "                prev_layer_val = self.ac.get_prev_layer_val().cpu().detach().cpu().numpy()\n",
    "                #print(o, prev_layer_val, a)\n",
    "                next_o, r, d, _ = self.env.step(a)\n",
    "                ep_ret += r\n",
    "                ep_len += 1\n",
    "                if get_env_action_space_type(self.env.action_space) == \"box\":\n",
    "                    lst_test_trajs.append(np.array(o.tolist()+prev_layer_val.tolist()+a.tolist()+[epoch, ep_ret, ep_len],dtype=float))\n",
    "                else:\n",
    "                    lst_test_trajs.append(np.array(o.tolist()+prev_layer_val.tolist()+[a[()]]+[epoch, ep_ret, ep_len],dtype=float))\n",
    "                o = next_o\n",
    "                terminal = d\n",
    "                if terminal:\n",
    "                    break\n",
    "            o, ep_ret, ep_len = self.env.reset(), 0, 0\n",
    "        print(\"epochs :\", epoch, \"ep_ret :\", ep_ret, \"ep_len :\", ep_len)\n",
    "        return lst_test_trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fc195b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_train_ppo(env, ppo_hid_nodes, seed, ppo_dir, ppo_act_fn, ppo_epochs, ppo_epoch_steps, ppo_gamma):\n",
    "    device=get_exec_device()\n",
    "    env.seed(seed)\n",
    "    env_obs_size, env_act_size = get_env_dims(env)\n",
    "\n",
    "    ppo_tb = None #TBWriter(ppo_dir)\n",
    "    ppo = PPO(env, actor_critic=MLPActorCritic, ac_kwargs=dict(hidden_sizes=ppo_hid_nodes), gamma=ppo_gamma, seed=seed, steps_per_epoch=ppo_epoch_steps, epochs=ppo_epochs, exp_dir=ppo_dir, device=device, act_fn=ppo_act_fn, tb=ppo_tb)\n",
    "    \n",
    "    if not os.path.exists(ppo_dir+\"/\"+\"ac_pi.mld\"):\n",
    "        lst_epochs_avg_ep_ret = ppo.ppo_train()\n",
    "        lst_epochs = list(range(0,len(lst_epochs_avg_ep_ret)))\n",
    "        save_list_as_csv([lst_epochs, lst_epochs_avg_ep_ret], ppo_dir, \"ppo_original_train_epoch_ret.csv\", columns=[\"epoch\", \"avg ep ret\"])\n",
    "    else:\n",
    "        ppo.load_ppo_model()\n",
    "        \n",
    "    ppo_weights, ppo_biases, ppo_std = ppo.get_ppo_actor_weights_std()\n",
    "    return ppo, ppo_weights, ppo_biases, ppo_std\n",
    "\n",
    "\n",
    "def fn_load_ppo(env, ppo_hid_nodes, seed, run_dir, ppo_dir, ppo_act_fn, ppo_epochs, ppo_epoch_steps, ppo_gamma):\n",
    "    device=get_exec_device()\n",
    "    env.seed(seed)\n",
    "    env_obs_size, env_act_size = get_env_dims(env)\n",
    "    ppo_tb = None\n",
    "    ppo = PPO(env, actor_critic=MLPActorCritic, ac_kwargs=dict(hidden_sizes=ppo_hid_nodes), gamma=ppo_gamma, seed=seed, steps_per_epoch=ppo_epoch_steps, epochs=ppo_epochs, exp_dir=ppo_dir, device=device, act_fn=ppo_act_fn, tb=ppo_tb)\n",
    "    \n",
    "    ppo.load_ppo_model()\n",
    "    \n",
    "    ppo_weights, ppo_biases, ppo_std = ppo.get_ppo_actor_weights_std()\n",
    "    print(\"ppo_weights\", ppo_weights)\n",
    "    print(\"ppo_biases\", ppo_biases)\n",
    "    print(\"ppo_std\", ppo_std)\n",
    "    return ppo, ppo_weights, ppo_biases, ppo_std\n",
    "    \n",
    "\n",
    "        \n",
    "def fn_evaluate_ppo(env, ppo, ppo_dir, evl_epochs,evl_max_epoch_steps, ppo_tb, device):\n",
    "    print(\"Evaluating PPO... Epochs:\", evl_epochs, \"  Steps per Epoch:\", evl_max_epoch_steps)\n",
    "    if not os.path.exists(ppo_dir+\"/\"+\"lst_ppo_eval_ep_ret.csv\"):\n",
    "        lst_ep_ret_ppo, lst_ep_len_ppo = evaluate_ppo(env=env, ppo=ppo, evl_epochs=evl_epochs, evl_max_epoch_steps=evl_max_epoch_steps, device=device)\n",
    "        np.savetxt(ppo_dir+\"/\"+\"lst_ppo_eval_ep_ret.csv\", np.array(lst_ep_ret_ppo), delimiter =\", \", fmt ='% s')\n",
    "        np.savetxt(ppo_dir+\"/\"+\"lst_ppo_eval_ep_len.csv\", np.array(lst_ep_len_ppo), delimiter =\", \", fmt ='% s')\n",
    "    else:\n",
    "        lst_ep_ret_ppo = np.loadtxt(ppo_dir+\"/\"+\"lst_ppo_eval_ep_ret.csv\", delimiter=\",\", dtype=float)\n",
    "        lst_ep_len_ppo = np.loadtxt(ppo_dir+\"/\"+\"lst_ppo_eval_ep_len.csv\", delimiter=\",\", dtype=float)\n",
    "    \n",
    "    if ppo_tb is not None:\n",
    "        for ep, (ppo_ret, ppo_len) in enumerate(zip(lst_ep_ret_ppo, lst_ep_len_ppo)):\n",
    "            ppo_tb.writer.add_scalar('Evaluation/Episode_Return', ppo_ret, ep)\n",
    "            ppo_tb.writer.add_scalar('Evaluation/Episode_Length', ppo_len, ep)\n",
    "        ppo_tb.writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3fae67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config_File_Name :  config_NCartPole_1.yaml\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m kwargs\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mncart\u001b[39m\u001b[39m\"\u001b[39m:\u001b[39m4\u001b[39m}\n\u001b[0;32m     29\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(env_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 30\u001b[0m obs_dim \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mobservation_space\u001b[39m.\u001b[39;49mshape\n\u001b[0;32m     31\u001b[0m act_dim \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mshape\n\u001b[0;32m     33\u001b[0m ppo_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mruns/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(env_name) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/ppo_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(ppo_hid_nodes) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/seed_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(seed)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--conf_file', type=str, default='config_NCartPole_1.yaml')\n",
    "args = parser.parse_args(\"\")\n",
    "print(\"Config_File_Name : \", args.conf_file)\n",
    "\n",
    "#Reading Experiment Configurations\n",
    "config_yaml = read_config(args.conf_file)\n",
    "config = Configs(config_yaml)\n",
    "ppo_act_fn = eval(config.ppo_act_fn)\n",
    "\n",
    "\n",
    "\n",
    "#Get the execution device\n",
    "run_count = 1\n",
    "run_dir = \"\"\n",
    "ppo_dir = \"\"\n",
    "exp_name = \"\"\n",
    "\n",
    "seed = config.aa_exp_seeds[0]\n",
    "ppo_hid_nodes = config.ppo_hid_nodes[0]\n",
    "env_name = config.env_name[0]\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "overwrite_flag = True\n",
    "\n",
    "kwargs={\"ncart\":4}\n",
    "env = gym.make(env_name, **kwargs)\n",
    "obs_dim = env.env.observation_space.shape\n",
    "act_dim = env.env.action_space.shape\n",
    "\n",
    "ppo_dir = \"runs/\" + str(env_name) + \"/ppo_\" + str(ppo_hid_nodes) + \"/seed_\" + str(seed)\n",
    "create_dir(ppo_dir)\n",
    "\n",
    "device = get_exec_device()\n",
    "\n",
    "#Original PPO\n",
    "org_ppo, ppo_weights, ppo_biases, ppo_std = fn_train_ppo(env, ppo_hid_nodes, seed, ppo_dir, ppo_act_fn, config.ppo_epochs, config.ppo_epoch_steps, config.ppo_gamma)\n",
    "fn_evaluate_ppo(env, org_ppo, ppo_dir, config.evl_epochs, config.evl_max_epoch_steps, None, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81616d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
